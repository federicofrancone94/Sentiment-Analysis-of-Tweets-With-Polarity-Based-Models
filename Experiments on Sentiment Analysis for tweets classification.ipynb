{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jn0q9aLybSRY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k95rzbrGbSRc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0D2ikHpybSRf",
    "outputId": "8945d0cc-1404-479e-fb8a-fe7c69f51ee0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\feder\\\\jupyter\\\\NLP course'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dxsE_0tAbSRj",
    "outputId": "3a99c350-1d45-4928-dd49-0e46b3073886"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1502"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets= []\n",
    "with open(\"dev_tweets.txt\") as f:\n",
    "    for line in f:\n",
    "        tweets.append(line.strip())\n",
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lwdl0ebwbSRl",
    "outputId": "f0a4b7e1-e85a-479d-8477-d7cb98618a67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2       , 1.22857143, 1.25714286, 1.28571429, 1.31428571,\n",
       "       1.34285714, 1.37142857, 1.4       ])"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(1.2,1.4,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GroDPCy-bSRo",
    "outputId": "92e337c3-f608-44fc-b906-1d2a64a7e0ee"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last Saturday ! SMILE_HAPPY #Happy Halloween !...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heat , Celtics , Bulls , knicks&amp;gt ; RT @Gama ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@TheTomHendricks could n't be without it now m...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good Saturday afternoon workout . By Myself ! ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@madgarden @rocketcatgames I 'll see if there ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet     label\n",
       "0  Last Saturday ! SMILE_HAPPY #Happy Halloween !...  positive\n",
       "1  Heat , Celtics , Bulls , knicks&gt ; RT @Gama ...   neutral\n",
       "2  @TheTomHendricks could n't be without it now m...  negative\n",
       "3  Good Saturday afternoon workout . By Myself ! ...  positive\n",
       "4  @madgarden @rocketcatgames I 'll see if there ...   neutral"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets= pd.DataFrame(tweets)\n",
    "tweets_labeled = pd.concat([df_tweets, pd.read_table(\"dev_categories.txt\", header= None)], axis=1)\n",
    "tweets_labeled.columns= ['tweet', 'label']\n",
    "tweets_labeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQfTODtIbSRr"
   },
   "source": [
    "### Importing the file of tweets parsed with CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58gFwJFqbSRr",
    "outputId": "b995e8ec-70bf-43e8-bb69-9ae4dad22730"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>entity</th>\n",
       "      <th>5</th>\n",
       "      <th>parent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Last</td>\n",
       "      <td>last</td>\n",
       "      <td>JJ</td>\n",
       "      <td>DATE</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>NNP</td>\n",
       "      <td>DATE</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>!</td>\n",
       "      <td>!</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>SMILE_HAPPY</td>\n",
       "      <td>SMILE_HAPPY</td>\n",
       "      <td>NNP</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>#Happy</td>\n",
       "      <td>#Happy</td>\n",
       "      <td>NNP</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Halloween</td>\n",
       "      <td>Halloween</td>\n",
       "      <td>NNP</td>\n",
       "      <td>DATE</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>!</td>\n",
       "      <td>!</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>SMILE_HAPPY</td>\n",
       "      <td>SMILE_HAPPY</td>\n",
       "      <td>NNP</td>\n",
       "      <td>MISC</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Can</td>\n",
       "      <td>can</td>\n",
       "      <td>MD</td>\n",
       "      <td>MISC</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>someone</td>\n",
       "      <td>someone</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>make</td>\n",
       "      <td>make</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>me</td>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>an</td>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>edit</td>\n",
       "      <td>edit</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Zayn</td>\n",
       "      <td>Zayn</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Malik</td>\n",
       "      <td>Malik</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>SMILE_HAPPY</td>\n",
       "      <td>SMILE_HAPPY</td>\n",
       "      <td>NNP</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Thanks</td>\n",
       "      <td>thanks</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>advance</td>\n",
       "      <td>advance</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>#hi</td>\n",
       "      <td>#hi</td>\n",
       "      <td>FW</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>LINK</td>\n",
       "      <td>link</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>Heat</td>\n",
       "      <td>heat</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        token        lemma  pos  entity  5 parent\n",
       "0      1         Last         last   JJ    DATE  _      0\n",
       "1      2     Saturday     Saturday  NNP    DATE  _      0\n",
       "2      3            !            !    .       O  _      0\n",
       "3      4  SMILE_HAPPY  SMILE_HAPPY  NNP       O  _      0\n",
       "4      5       #Happy       #Happy  NNP       O  _      0\n",
       "5      6    Halloween    Halloween  NNP    DATE  _      0\n",
       "6      7            !            !    .       O  _      0\n",
       "7      8  SMILE_HAPPY  SMILE_HAPPY  NNP    MISC  _      0\n",
       "8      9          Can          can   MD    MISC  _      0\n",
       "9     10      someone      someone   NN       O  _      0\n",
       "10    11         make         make   VB       O  _      0\n",
       "11    12           me            I  PRP       O  _      0\n",
       "12    13           an            a   DT       O  _      0\n",
       "13    14         edit         edit   NN       O  _      0\n",
       "14    15           of           of   IN       O  _      0\n",
       "15    16         Zayn         Zayn  NNP  PERSON  _      0\n",
       "16    17        Malik        Malik  NNP  PERSON  _      0\n",
       "17    18            ?            ?    .       O  _      0\n",
       "18    19  SMILE_HAPPY  SMILE_HAPPY  NNP       O  _      0\n",
       "19    20       Thanks       thanks  NNS       O  _      0\n",
       "20    21           in           in   IN       O  _      0\n",
       "21    22      advance      advance   NN       O  _      0\n",
       "22    23            .            .    .       O  _      0\n",
       "23    24          #hi          #hi   FW       O  _      0\n",
       "24    25         LINK         link   NN       O  _      0\n",
       "25     1         Heat         heat   NN       O  _      0"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed= pd.read_table(\"dev_tweets_parsed.txt\", header= None)\n",
    "parsed.columns= [['index', 'token', 'lemma', 'pos', 'entity', 5, 'parent']]\n",
    "parsed.head(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35WivrazbSRz"
   },
   "source": [
    "# Sentiment Analysis using NLTK built-in libraries (skip this section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vkgSdmpYbSR0",
    "outputId": "c81bb14d-735b-45bf-cdef-2da8a56db35f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feder\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZKFbF9PbSR2"
   },
   "outputs": [],
   "source": [
    "\"\"\"SETTING UPPER BOUND AND LOWER BOUND SCORES FOR NEUTRAL TWEETS\"\"\"\n",
    "ub= 0.34\n",
    "lb= -0.34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-i-H35NybSR5",
    "outputId": "2e00a03a-4560-45a1-f559-07e5319f3518"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last Saturday ! SMILE_HAPPY #Happy Halloween !...</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.852, 'pos': 0.148, 'comp...</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heat , Celtics , Bulls , knicks&amp;gt ; RT @Gama ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@TheTomHendricks could n't be without it now m...</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.906, 'pos': 0.094, 'comp...</td>\n",
       "      <td>0.3365</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good Saturday afternoon workout . By Myself ! ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.842, 'pos': 0.158, 'comp...</td>\n",
       "      <td>0.4926</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@madgarden @rocketcatgames I 'll see if there ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet     label  \\\n",
       "0  Last Saturday ! SMILE_HAPPY #Happy Halloween !...  positive   \n",
       "1  Heat , Celtics , Bulls , knicks&gt ; RT @Gama ...   neutral   \n",
       "2  @TheTomHendricks could n't be without it now m...  negative   \n",
       "3  Good Saturday afternoon workout . By Myself ! ...  positive   \n",
       "4  @madgarden @rocketcatgames I 'll see if there ...   neutral   \n",
       "\n",
       "                                              scores  compound predicted  \n",
       "0  {'neg': 0.0, 'neu': 0.852, 'pos': 0.148, 'comp...    0.5399  positive  \n",
       "1  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   neutral  \n",
       "2  {'neg': 0.0, 'neu': 0.906, 'pos': 0.094, 'comp...    0.3365   neutral  \n",
       "3  {'neg': 0.0, 'neu': 0.842, 'pos': 0.158, 'comp...    0.4926  positive  \n",
       "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   neutral  "
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_labeled = pd.concat([df_tweets, pd.read_table(\"dev_categories.txt\", header= None)], axis=1)\n",
    "tweets_labeled.columns= ['tweet', 'label']\n",
    "\n",
    "tweets_labeled['scores'] = tweets_labeled['tweet'].apply(lambda review: sid.polarity_scores(review))\n",
    "\n",
    "tweets_labeled['compound']  = tweets_labeled['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "tweets_labeled['predicted'] = tweets_labeled['compound'].apply(lambda c: 'positive' if c >ub else ('negative' if c<lb else 'neutral'))\n",
    "\n",
    "tweets_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "77zuO7-JbSR7",
    "outputId": "53bc14c7-71d3-4a03-add8-6c17a08c3abb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9337, 0.9725)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= tweets_labeled\n",
    "df['compound'].min(), df['compound'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hV7ANHX-bSR-"
   },
   "source": [
    "#### DISTIRBUTION OF NEUTRAL COMPOUNDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gaonm1QabSR-",
    "outputId": "c7648a86-2fa5-4b4c-a036-cc8ecd366122"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  9.,  19.,  25.,  43., 305.,  40.,  68.,  93.,  47.,  22.]),\n",
       " array([-0.9337 , -0.74464, -0.55558, -0.36652, -0.17746,  0.0116 ,\n",
       "         0.20066,  0.38972,  0.57878,  0.76784,  0.9569 ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(df[ (df['label']=='neutral')]['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6flVMKkxbSSA",
    "outputId": "2bbcee5a-e76c-4aef-a572-8784ef73e9c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6438152011922503"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= tweets_labeled\n",
    "len(df[ (df['label']=='neutral') & (df['predicted']== 'neutral')])/len(df[df['label']=='neutral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ELq_46m4bSSC",
    "outputId": "f6894a30-4d75-45a3-b2fc-587b4ac57ef9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7108792846497765"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall= len(df[ (df['label']=='neutral') & (abs(df['compound']< ub ))])/len(df[df['label']=='neutral'])\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DbL0JqspbSSF",
    "outputId": "94a49513-f0d5-4a95-b680-559bb841f291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.6158455392809588, F1 measure is 0.6109458116911621\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.61      0.42      0.50       307\n",
      "    neutral       0.62      0.64      0.63       671\n",
      "   positive       0.61      0.70      0.65       524\n",
      "\n",
      "avg / total       0.62      0.62      0.61      1502\n",
      "\n",
      "\n",
      "\n",
      "[[128 128  51]\n",
      " [ 58 432 181]\n",
      " [ 23 136 365]]\n"
     ]
    }
   ],
   "source": [
    "df= tweets_labeled\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix, recall_score, f1_score\n",
    "def scores_df (df, true= 'label', pred= 'predicted'):\n",
    "    print('accuracy is {}, F1 measure is {}'.format(accuracy_score(df[true],df[pred]), f1_score(df[true],df[pred], average= 'weighted')))\n",
    "    print('\\n')\n",
    "    print(classification_report(df[true],df[pred]))\n",
    "    print('\\n')\n",
    "    print(confusion_matrix(df[true],df[pred]))\n",
    "\n",
    "scores_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hvv7R_p9bSSH",
    "outputId": "b3c14607-65c4-4d1f-a1ca-5cb92bab5f90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.6035359371388953, F1 measure is 0.6009158809123211\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.48      0.39      0.43      1279\n",
      "    neutral       0.63      0.63      0.63      4131\n",
      "   positive       0.61      0.65      0.63      3244\n",
      "\n",
      "avg / total       0.60      0.60      0.60      8654\n",
      "\n",
      "\n",
      "\n",
      "[[ 504  563  212]\n",
      " [ 377 2597 1157]\n",
      " [ 168  954 2122]]\n"
     ]
    }
   ],
   "source": [
    "tweets_train = []\n",
    "with open(\"train_tweets.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tweets_train.append(line.strip())\n",
    "        \n",
    "df= pd.DataFrame(tweets_train)\n",
    "tweets_labeled = pd.concat([df, pd.read_table(\"train_categories.txt\", header= None)], axis=1)\n",
    "tweets_labeled.columns= ['tweet', 'label']\n",
    "\n",
    "\n",
    "tweets_labeled['scores'] = tweets_labeled['tweet'].apply(lambda review: sid.polarity_scores(review))\n",
    "\n",
    "tweets_labeled['compound']  = tweets_labeled['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "\n",
    "tweets_labeled['predicted'] = tweets_labeled['compound'].apply(lambda c: 'positive' if c >ub else ('negative' if c<lb else 'neutral'))\n",
    "\n",
    "tweets_labeled.head()\n",
    "\n",
    "scores_df(tweets_labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9H7h2tgqbSSM"
   },
   "source": [
    "# BOW MODEL \n",
    "1. First classical, with term frequency (CountVectorizer..) \n",
    "2. then TF-IDF\n",
    "3. then TF-IDF modified with polarity terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eBXgP9eubSSM"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tweets_labeled = pd.concat([df_tweets, pd.read_table(\"dev_categories.txt\", header= None)], axis=1)\n",
    "tweets_labeled.columns= ['tweet', 'label']\n",
    "df= tweets_labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJfmN30ibSSO"
   },
   "source": [
    "**I do not consider punctuation as lemmas, so I take it off from tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q1w6wXBvbSSP",
    "outputId": "5e425293-2f19-4b46-cd00-137e43a57d4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"', '$', '%', '&', \"'\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"IMPORTANTE: mantengo ! e #, + e - --> è una scelta\"\"\"\n",
    "import string\n",
    "punct= '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' \n",
    "punct= [el for el in punct if el not in ['!', \"#\", '+', '-']]\n",
    "punct[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "glzaCk97bSSR",
    "outputId": "b2e67b7d-2150-4c1a-bdd9-07d3c64f2395"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"$%&\\'()*,./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UeWAgPj0bSSU"
   },
   "outputs": [],
   "source": [
    "df['tweet']= df['tweet'].apply(lambda x: x.strip(''.join(punct))) #tolgo la punctuation ai tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I91joqGsbSSW"
   },
   "outputs": [],
   "source": [
    "X = df['tweet']  # this time we want to look at the text\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) #X_train ha 1006 elementi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fwJTgMQhbSSZ",
    "outputId": "fd1fe46e-582d-41f7-baa7-988e9cc372f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1006, 4429)"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Inutile, CountVectorizer è il BOW semplice basato sulle frequenze..pessimo per la SA\"\"\"\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)  #se aggiungo to_dense() o toarray() posso visualizzare matrice\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6vnJIhbhbSSb",
    "outputId": "a8cbf2cd-e2a7-495a-9415-7f7c47ec3790"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.toarray()  #la max frequenza è 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0jX3JSJbSSe",
    "outputId": "8318a2c2-239e-445a-c678-cbbf2322a716"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thi', 'movi', 'is', 'wonder']"
      ]
     },
     "execution_count": 936,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Stemming\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "words = ['this','movie','is','wonderful']\n",
    "words = [ps.stem(x) for x in words]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f5lkRHBlbSSg"
   },
   "outputs": [],
   "source": [
    "def scores (true, pred):\n",
    "    \"\"\"ACCURACY, F1-MEASURE, CONFUSION MATRIX E CLASSIFICATION REPORT\"\"\"\n",
    "    print('accuracy is {}, F1 measure is {}'.format(accuracy_score(true,pred), f1_score(true,pred, average= 'weighted')))\n",
    "    print('\\n')\n",
    "    print(classification_report(true,pred))\n",
    "    print('\\n')\n",
    "    print(confusion_matrix(true,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UnVfMHnAbSSi"
   },
   "source": [
    "### Create function that given a sentence, returns the POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZUNYMdCbSSj",
    "outputId": "1b0ce2ef-057b-4f2a-8844-2c263efbd977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Last', 'JJ'), ('Saturday', 'NNP'), ('!', '.'), ('SMILE_HAPPY', 'NNP'), ('#', '#'), ('Happy', 'NNP'), ('Halloween', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize, TreebankWordTokenizer, sent_tokenize    \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer= TreebankWordTokenizer()\n",
    "\n",
    "#nltk.download('wordnet')   <-- DECOMMENTALA E INSTALLALO!\n",
    "print(nltk.pos_tag(nltk.word_tokenize(X_train[0])[:7])) #prova per vedere se fa il POS di una frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eDNhFQD_bSSk",
    "outputId": "f4cbbf3f-58a1-451b-fbf6-0c113e39b592"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# faccio una prova\n",
    "word = 'going'\n",
    "lemmatizer.lemmatize(word, 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing some Trials: I must do POS on the tokenized sentence on a list. It can't be done on a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hZ_cHSLYbSSo",
    "outputId": "62e4b6ce-39c5-4d74-a152-1f14162df526"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'), ('love', 'VBP'), ('you', 'PRP')]"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1= ['I', 'love', 'you']\n",
    "s2= ['My', 'love', 'is', 'intense']\n",
    "\n",
    "nltk.pos_tag(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EM5WwLG9bSSq",
    "outputId": "6ebbdd5a-a4d5-4746-a40c-df8a1442d156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP')]\n",
      "[('love', 'NN')]\n",
      "[('you', 'PRP')]\n"
     ]
    }
   ],
   "source": [
    "for elem in s1:\n",
    "    print(nltk.pos_tag([elem]))   #qui 'love' me lo da come Noun!!! ERRRORE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oO1gvTgtbSSs"
   },
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convertion between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ    # 'a', that is 'adjective'\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN   # 'n', that is 'noun'\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV    # 'r', that is 'adverb'\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB   #  'v', that is 'verb'\n",
    "    return 'n'   #I put noun by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wk3lEQEwbSSu",
    "outputId": "9394cb0d-90d7-494c-a119-9752580b5674"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last\n",
      "Saturday\n",
      "!\n",
      "SMILE_HAPPY\n",
      "#\n",
      "Happy\n",
      "Halloween\n"
     ]
    }
   ],
   "source": [
    "for elem in nltk.pos_tag(TreebankWordTokenizer().tokenize(X_train[0])[:7]):\n",
    "    print(lemmatizer.lemmatize(elem[0], penn_to_wn(elem[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e7VcXp8BbSSw"
   },
   "source": [
    "## LEMMATIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I do the POS of each word and I put the lemma in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l1GWbi4PbSSw"
   },
   "outputs": [],
   "source": [
    "def tokenize_tag(sentence):\n",
    "    \"\"\"IT WORKS BOTH WITH A STRING AND A LIST OF WORDS. IT RETURNS A SET OF LEMMAS\"\"\"\n",
    "    if type(sentence)== str:\n",
    "        list_tokens = tokenizer.tokenize(sentence)\n",
    "    elif type(sentence)==list:\n",
    "        string= ' '.join(sentence)\n",
    "        list_tokens= tokenizer.tokenize(string)\n",
    "        \n",
    "    tagged_sent= nltk.pos_tag(list_tokens)\n",
    "    \"\"\"TAGGING WITH Wordnet Notation ('a', 'n', 'v' and 'r')\"\"\"\n",
    "    \"\"\" tagged_wn= []\n",
    "    for item in tagged_sent:\n",
    "        tagged_wn.append((item[0].lower(), penn_to_wn(item[1])))\n",
    "        \n",
    "    return tagged_wn\"\"\"\n",
    "    return tagged_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0E7ru2YbSSz",
    "outputId": "b8fbe69d-2ae9-4215-cf6d-3830118138d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('you', 'PRP'), ('are', 'VBP'), ('my', 'PRP$'), ('love', 'NN')],\n",
       " [('i', 'NN'), ('love', 'VBP'), ('you', 'PRP')])"
      ]
     },
     "execution_count": 155,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_tag('you are my love'), tokenize_tag('i love you')  #corretto, love è prima verbo poi nome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "STmDwmN1bSS1"
   },
   "outputs": [],
   "source": [
    "def tokenize_tag_lemmatize(sentence):\n",
    "    \"\"\"TOKENIZING AND LEMMATIZING\"\"\"\n",
    "    tagged_sent= tokenize_tag(sentence)\n",
    "    \"\"\"LEMMATIZING\"\"\"\n",
    "    lemmas= []\n",
    "    for item in tagged_sent:\n",
    "        lemma= lemmatizer.lemmatize(item[0].lower(), penn_to_wn(item[1]))\n",
    "        #lemmas.add(lemma)\n",
    "        lemmas.append(lemma)\n",
    "        \n",
    "    return set(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2F9w2LfbSS3"
   },
   "outputs": [],
   "source": [
    "from log_progess import log_progress\n",
    "def bag_of_lemmas(tweets):\n",
    "    \"\"\"IL SET MI GARANTISCE CHE ELEMENTI SIANO UNICI\"\"\"\n",
    "    list_of_lemmas=[]\n",
    "    count= 0\n",
    "    for i in log_progress(tweets.index, every=1):\n",
    "        sent= X_train[i]\n",
    "        lemmas = list(tokenize_tag_lemmatize(sent))\n",
    "        \n",
    "        list_of_lemmas+=lemmas\n",
    "    return set(list_of_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5M5Ix_zabSS5",
    "outputId": "6278d450-f2d4-428e-9a8f-7cf5d41223f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['kk', 'competition', 'domino', 'market', 'result', 'brittania'], 4157)"
      ]
     },
     "execution_count": 156,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_lemmas = bag_of_lemmas(X_train)\n",
    "terms= list(list_of_lemmas)\n",
    "terms[:6], len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yk-a07dpbSS7",
    "outputId": "32e145fc-c018-4189-bcc1-122cf6132779"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4157"
      ]
     },
     "execution_count": 157,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terms)   #4157 terms over 1006 tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E3DZo8t8bSS-"
   },
   "source": [
    "**Now I have the Bag of Lemmas: I have to create the Matrix Lemma x Tweets with the frequencies. Then I will transform in TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-84b-w_lbSS_"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(vocabulary= list(terms), stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uenOXjfwbSTC"
   },
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pPZCW7NqbSTE"
   },
   "outputs": [],
   "source": [
    "diz_terms= tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdvpUgCAbSTJ",
    "outputId": "911b0aad-df9e-4837-ff52-5eeed4d9d65c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 192,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### converting to dense matrix\n",
    "\n",
    "def_tfidf_lemmat.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTIoyn-ObSTM"
   },
   "source": [
    "## Working on Polarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHjeu-hdbSTN",
    "outputId": "ed3f8d9a-0dd2-4f1d-9642-cbbac97c35cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([SentiSynset('happy.a.01'),\n",
       "  SentiSynset('felicitous.s.02'),\n",
       "  SentiSynset('glad.s.02'),\n",
       "  SentiSynset('happy.s.04')],\n",
       " [SentiSynset('decelerate.v.01'),\n",
       "  SentiSynset('slow.v.02'),\n",
       "  SentiSynset('slow.v.03'),\n",
       "  SentiSynset('slow.a.01'),\n",
       "  SentiSynset('slow.a.02'),\n",
       "  SentiSynset('dense.s.04'),\n",
       "  SentiSynset('slow.a.04'),\n",
       "  SentiSynset('boring.s.01'),\n",
       "  SentiSynset('dull.s.08'),\n",
       "  SentiSynset('slowly.r.01'),\n",
       "  SentiSynset('behind.r.03')])"
      ]
     },
     "execution_count": 162,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "#nltk.download('sentiwordnet')\n",
    "\n",
    "list(swn.senti_synsets('happy' , 'a')), list(swn.senti_synsets('slow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gzos8np1bSTO"
   },
   "outputs": [],
   "source": [
    "def fin_get_sentiment(word,tag):\n",
    "    \"\"\"tag must be 'a', 'n', 'r' or 'v' \n",
    "    PER SAPERE POLARITY DI UNA PAROLA, GUARDO TUTTI I SINONIMI E FACCIO POLARITY MEDIA DI QUELLI. \n",
    "    Non c'è possibilità di sapere polarity di una parola così com'è...\"\"\"\n",
    "    \n",
    "    \n",
    "    synsets= list(swn.senti_synsets(word,tag))\n",
    "    \n",
    "    diz= {'pos': 0, 'neg': 0, 'neutral':0}\n",
    "    \n",
    "    lb= 0.33\n",
    "    #ub= 0.66\n",
    "    num_synsets= len(synsets)\n",
    "    \n",
    "    if num_synsets == 0:\n",
    "        return {word: []}\n",
    "    \n",
    "    elif num_synsets >0:\n",
    "        for syn in synsets:\n",
    "            if (syn.pos_score()>lb or syn.neg_score()>lb):\n",
    "                diz['pos']+= syn.pos_score()\n",
    "                diz['neg']+= syn.neg_score()\n",
    "                #diz['neutral']+= syn.obj_score()* lb / ub\n",
    "                \n",
    "            else:\n",
    "                if num_synsets >=2:\n",
    "                    num_synsets= num_synsets -1   #se una parola ha polarità neutra, non la conto per farci la media..\n",
    "                \n",
    "    for key in diz:\n",
    "        diz[key]/= num_synsets\n",
    "    \n",
    "    norm= sum(diz.values())\n",
    "    \n",
    "    #if norm>0:\n",
    "    \n",
    "    for key in diz:\n",
    "        #diz[key] /= norm\n",
    "        diz[key] = round(diz[key],2)\n",
    "    \n",
    "    #print(word, diz)\n",
    "        \n",
    "    return {word: diz}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A0Zp7YWubSTQ"
   },
   "outputs": [],
   "source": [
    "def get_sentiment_sentence(sentence):\n",
    "    \"\"\"IT WORKS BOTH WITH A STRING AND A LIST OF WORDS\"\"\"\n",
    "    tagged_sent= tokenize_tag(sentence)\n",
    "    \n",
    "    total_scores= {'pos': 0, 'neg': 0, 'neutral':0}\n",
    "    \n",
    "    num_tokens = len(tagged_sent)\n",
    "    #print('num_tokens è: ', num_tokens)\n",
    "    \n",
    "    lb= 0.33\n",
    "    #ub= 0.66\n",
    "    \n",
    "    for item in tagged_sent:\n",
    "        d= fin_get_sentiment(item[0], penn_to_wn(item[1]))\n",
    "        #print('dizionario è: ', d)\n",
    "        scores = d[item[0]]\n",
    "        #print(scores)\n",
    "        if len(scores)>0:\n",
    "            #print('C\\'è un dizionario di valori pos neg e neutral per questa parola')\n",
    "            if scores['pos']>lb or scores['neg']>lb:\n",
    "                total_scores['pos']+= scores['pos']\n",
    "                total_scores['neg']+= scores['neg']\n",
    "                #total_scores['neutral']+= scores['neutral']\n",
    "            else:\n",
    "                num_tokens= num_tokens -1\n",
    "        else:\n",
    "                num_tokens= num_tokens -1\n",
    "\n",
    "        \n",
    "    #norm= sum(total_scores.values())\n",
    "    \n",
    "    #if norm>0:\n",
    "    #print('PER QUANTO DIVIDO?', num_tokens)\n",
    "    if num_tokens>0:\n",
    "        for key in total_scores:\n",
    "            #total_scores[key] /= norm\n",
    "            total_scores[key] /= num_tokens\n",
    "            total_scores[key] = round(total_scores[key],2)\n",
    "    \n",
    "    \n",
    "    #return {'pos': ssynset.pos_score(), 'neg': swn_synset.neg_score(), 'neutral': swn_synset.obj_score()}\n",
    "    return total_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U6H62tv-bSTS",
    "outputId": "b3f29d93-c347-439e-899a-145e7024978c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'love': {'pos': 0.62, 'neg': 0.03, 'neutral': 0.0}},\n",
       " {'amazing': {'pos': 0.69, 'neg': 0.19, 'neutral': 0.0}})"
      ]
     },
     "execution_count": 178,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_get_sentiment('love', 'v'), fin_get_sentiment('amazing', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7DXytoWbSTT",
    "outputId": "9144bd8a-2178-4563-ba63-c423dbf310ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos': 0.81, 'neg': 0.0, 'neutral': 0.0}"
      ]
     },
     "execution_count": 188,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment_sentence('The film was great')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CdYLns2ebSTX"
   },
   "source": [
    "# Reducing dimension with LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9YuezvQbSTX",
    "outputId": "a051ce49-1b84-4b68-8481-f0de97cc410b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  done in 8.263sec\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "svd = TruncatedSVD(400)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "X_train_lsa = lsa.fit_transform(def_tfidf_lemmat)\n",
    "print(\"  done in %.3fsec\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mEDe40xybSTa"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ATVhyq_bSTd",
    "outputId": "d4e3dcd2-66ca-4edd-d709-d5870417f7b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Explained variance of the SVD step: 69%\n"
     ]
    }
   ],
   "source": [
    "# Run SVD on the training data, then project the training data.\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"  Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "el_edmHtbSTg"
   },
   "outputs": [],
   "source": [
    "# Now apply the transformations to the test data as well.\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "X_test_lsa = lsa.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ye1cMF0bSTi"
   },
   "outputs": [],
   "source": [
    "knn_lsa = KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='cosine')\n",
    "knn_lsa.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Classify the test vectors.\n",
    "p = knn_lsa.predict(tfidf_vectorizer.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VowaxvAgbSTr"
   },
   "source": [
    "**Finally, once the matrix is correctly weighted (and/or reduced), I can apply any classification algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vvk4aifqbSTs"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, validation_curve, KFold\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "svm = SVC()\n",
    "\"\"\"param_grid = [\n",
    "  {'C': [0.95, 0.9, 1, 1.05, 1.1], 'kernel': ['linear']},\n",
    "  {'C': [1, 5, 10], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}, ]\"\"\"\n",
    "\n",
    "param_grid = [\n",
    "  {'C': np.linspace(0.75, 1.25, 7), 'kernel': ['linear']}]\n",
    "#parameters = {'kernel':('linear', 'rbf','poly', 'sigmoid'), 'C':(1, 10, 100),'gamma': (0.001, 0.0001,'auto'),'shrinking':(True,False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGtebNqvbSTt",
    "outputId": "37831352-35ce-430f-bd54-af8c7e53a6cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feder\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([('tfidf', TfidfVectorizer(stop_words= 'english', vocabulary= list(terms))),\n",
    "                     ('clf', GridSearchCV(svm, param_grid)),\n",
    "])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_train, y_train)  \n",
    "predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0neZw32EbSTv",
    "outputId": "ff713671-847e-41cf-d854-8dbcbd7a74d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0833333333333333, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 1377,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.named_steps['clf'].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6RKSvC6FbSTx",
    "outputId": "093f854e-39dc-4a7f-e749-c83ba9b2c927"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.5504032258064516, F1 measure is 0.5324715135574091\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.25      0.35       110\n",
      "     neutral       0.54      0.73      0.62       217\n",
      "    positive       0.56      0.51      0.53       169\n",
      "\n",
      "   micro avg       0.55      0.55      0.55       496\n",
      "   macro avg       0.55      0.50      0.50       496\n",
      "weighted avg       0.55      0.55      0.53       496\n",
      "\n",
      "\n",
      "\n",
      "[[ 28  61  21]\n",
      " [ 12 159  46]\n",
      " [ 11  72  86]]\n"
     ]
    }
   ],
   "source": [
    "scores(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbNUuzrgbSTV"
   },
   "source": [
    "### Next Experiment: modifying weights of TF-IDF with polarities of words in the tweets...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".............................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP Project cleaned.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
