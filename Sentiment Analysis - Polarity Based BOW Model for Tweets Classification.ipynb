{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In questa versione estendo il discorso delle polarity agganciandomi anche a SentiWordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the file with the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "from parsed_sentence import *\n",
    "\n",
    "def load_classes(input_file_path):\n",
    "    res = []\n",
    "    with open(input_file_path, encoding=\"utf-8\") as fp:\n",
    "        for line in fp:\n",
    "            res.append(line.strip())\n",
    "    return res\n",
    "\n",
    "def load_file(input_file_path):\n",
    "    res = []\n",
    "    psr = ParsedSentenceReader(input_file_path)\n",
    "    while True:\n",
    "        ps = psr.read_next_sentence()\n",
    "        if not ps:\n",
    "            break\n",
    "        res.append(ps)\n",
    "    return res\n",
    "\n",
    "\n",
    "    \n",
    "#====== MAIN =============================================\n",
    "\n",
    "if not len(sys.argv) == 3:\n",
    "    print(\"Usage: class_file text_file\")\n",
    "    exit()\n",
    "\n",
    "#TRAIN\n",
    "#input_file_path1= 'C:/Users/feder/Desktop/Master/Project_NLP/resource/semeval2013/train_categories.txt'\n",
    "#input_file_path2= 'C:/Users/feder/Desktop/Master/Project_NLP/resource/semeval2013/train_tweets_parsed.txt'\n",
    "\n",
    "#DEV\n",
    "input_file_path1= 'C:/Users/feder/Desktop/Master/Project_NLP/resource/semeval2013/dev_categories.txt'\n",
    "input_file_path2= 'C:/Users/feder/Desktop/Master/Project_NLP/resource/semeval2013/dev_tweets_parsed.txt'\n",
    "\n",
    "class_array = load_classes(input_file_path1)\n",
    "parsed_sent_array = load_file(input_file_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parsed_sent_array) == len(class_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING POLARITY LEXICON TO ASSIGN TO EACH WORD A POLARITY SCORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://sag.art.uniroma2.it/demo-software/distributional-polarity-lexicon/\n",
    "- positive, negative and neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>0.17247152,0.6118044,0.21572405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!</td>\n",
       "      <td>0.4407992,0.22381033,0.33539042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i</td>\n",
       "      <td>0.15485209,0.7418242,0.10332363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>0.30322838,0.27008215,0.42668942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>,</td>\n",
       "      <td>0.5385329,0.29464877,0.16681834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                 1\n",
       "0  </s>   0.17247152,0.6118044,0.21572405\n",
       "1     !   0.4407992,0.22381033,0.33539042\n",
       "2     i   0.15485209,0.7418242,0.10332363\n",
       "3   the  0.30322838,0.27008215,0.42668942\n",
       "4     ,   0.5385329,0.29464877,0.16681834"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the Polarity Lexicon\n",
    "polwords = pd.read_csv(\"\"\"DPL-EN_lrec2016.txt\"\"\",delimiter='\\t', header=None)\n",
    "polwords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "polaritywords = {}\n",
    "for i in range(len(polwords)):\n",
    "    polaritywords[polwords.iloc[i,0]]=list(map(float, polwords.iloc[i,1].split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.22627498, 0.708745, 0.06498006]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### For each word there are three scores: positive polarity, neutral and negative\n",
    "\n",
    "polaritywords['remembered']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting all the smiles types from all the tweets in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8654"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train = []\n",
    "with open(\"train_tweets.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tweets_train.append(line.strip())\n",
    "len(tweets_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uh drunk and tired and just remembered i 'm go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@theboltonnews pls rt - bolton society for bli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  uh drunk and tired and just remembered i 'm go...\n",
       "1  @theboltonnews pls rt - bolton society for bli..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_train = pd.DataFrame(tweets_train).apply(lambda x: [word.lower() for word in x])\n",
    "df_tweets_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'smile_cry',\n",
       " 'smile_disgust',\n",
       " 'smile_embarassed',\n",
       " 'smile_happy',\n",
       " 'smile_kiss',\n",
       " 'smile_laugh',\n",
       " 'smile_love',\n",
       " 'smile_sad',\n",
       " 'smile_skeptical',\n",
       " 'smile_surprise',\n",
       " 'smile_tongue',\n",
       " 'smile_wink'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "smiles= []\n",
    "for i in range(len(tweets_train)):\n",
    "    text= df_tweets_train.iloc[i,:].values[0]\n",
    "    match= re.search(r'smile_\\w+', text)\n",
    "    if match is not None:\n",
    "        smiles.append(match.group())\n",
    "smiles= set(smiles)\n",
    "smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning to each of these smiles a polarity, modifying that manually\n",
    "- **FOR SMILE_CRY, SMILE_HAPPY, SMILE_SAD, SMILE_DISGUST AND SMILE_LOVE, I CHOOSE TO PUT 100% OF POLARITY IN ONE OF THE 3 SCORES, GIVEN THE FACT THEY WOULD BE HIGHLY CONNECTED TO THE SENTIMENT OF THE SENTENCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smile_happy \t\t [1, 0, 0]\n",
      "smile_cry \t\t [0, 1, 0]\n",
      "smile_sad \t\t [0, 1, 0]\n",
      "smile_disgust \t\t [0, 0, 1]\n",
      "smile_love \t\t [1, 0, 0]\n",
      "smile_skeptical [0.2750377, 0.2475464, 0.47741592]\n",
      "smile_surprise \t [0.3524347, 0.1533607, 0.49420467]\n",
      "smile_wink \t [0.54961383, 0.20071952, 0.24966662]\n",
      "smile_tongue \t [0.18763916, 0.3646318, 0.44772902]\n",
      "smile_laugh \t [0.55425787, 0.11650112, 0.32924098]\n",
      "smile_embarassed [0.11972178, 0.6342528, 0.2460254]\n",
      "smile_kiss \t [0.262494, 0.31871325, 0.4187927]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "smiles= []\n",
    "for i in range(len(tweets_train)):\n",
    "    text= df_tweets_train.iloc[i,:].values[0]\n",
    "    match= re.search(r'smile_\\w+', text)\n",
    "    if match is not None:\n",
    "        smiles.append(match.group())\n",
    "smiles= set(smiles)\n",
    "smiles\n",
    "\n",
    "for smile in smiles:\n",
    "    type_smile= smile[len('smile_'):]\n",
    "    if type_smile in ['happy', 'cry', 'sad', 'disgust', 'love']:\n",
    "        #initialize with zero polarities\n",
    "        polaritywords[smile]= [0, 0, 0]  \n",
    "        max_index_polarity= np.argmax(polaritywords.get(type_smile))\n",
    "        polaritywords[smile][max_index_polarity]= 1\n",
    "    else:\n",
    "        polaritywords[smile]= polaritywords.get(type_smile)\n",
    "    print(smile, '\\t', polaritywords[smile])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to SentiWordnet to get synonimous of each word and the related polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([SentiSynset('happy.a.01'),\n",
       "  SentiSynset('felicitous.s.02'),\n",
       "  SentiSynset('glad.s.02'),\n",
       "  SentiSynset('happy.s.04')],\n",
       " [SentiSynset('decelerate.v.01'),\n",
       "  SentiSynset('slow.v.02'),\n",
       "  SentiSynset('slow.v.03'),\n",
       "  SentiSynset('slow.a.01'),\n",
       "  SentiSynset('slow.a.02'),\n",
       "  SentiSynset('dense.s.04'),\n",
       "  SentiSynset('slow.a.04'),\n",
       "  SentiSynset('boring.s.01'),\n",
       "  SentiSynset('dull.s.08'),\n",
       "  SentiSynset('slowly.r.01'),\n",
       "  SentiSynset('behind.r.03')])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "list(swn.senti_synsets('happy' , 'a')), list(swn.senti_synsets('slow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ    # 'a','adjective'\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN   # 'n',  'noun'\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV    # 'r',  'adverb'\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB   # 'v', 'verb'\n",
    "    return 'n'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_sentiwordnet(word,tag, lb= 0, sinonimi= 'half'):\n",
    "    \"\"\" tag must be 'a', 'n', 'r' or 'v'\n",
    "    \n",
    "    sinonimi can be 'average' or n° or 'half'\n",
    "    If 'average':\n",
    "    to know a word polarity, I look at its synonymous and I compute average polarity for those ones.\n",
    "    Note: you cannot get the polarity of one single word from sentiwordnet library\n",
    "    \n",
    "    \n",
    "    If numero: I look only at the first synonyms closer to the word and I take polarity of that one\n",
    "    If 'half': I comput average of first half of synonymous\"\"\"\n",
    "    \n",
    "    \n",
    "    diz= {'pos': 0, 'neg': 0, 'neutral':0}\n",
    "    lb= 0\n",
    "    #ub= 0.66\n",
    "    \n",
    "    if sinonimi== 'average':\n",
    "        synsets= list(swn.senti_synsets(word,tag))\n",
    "        \n",
    "    elif sinonimi== 'half':\n",
    "        try:\n",
    "            n_sin= len(list(swn.senti_synsets(word,tag)))\n",
    "            half= round(n_sin/2)\n",
    "\n",
    "            synsets= list(swn.senti_synsets(word,tag))[:half]\n",
    "            \n",
    "        except:  \n",
    "            diz['pos']+= swn.senti_synsets(word,tag).pos_score()\n",
    "            diz['neg']+= swn.senti_synsets(word,tag).neg_score()\n",
    "            diz['neutral']+= swn.senti_synsets(word,tag).obj_score()\n",
    "            return dix\n",
    "            \n",
    "    \n",
    "    else:  \n",
    "        try:\n",
    "            if len(list(swn.senti_synsets(word,tag)))> sinonimi:  \n",
    "                synsets= list(swn.senti_synsets(word,tag))[:sinonimi] \n",
    "            else:\n",
    "                n_sin= len(list(swn.senti_synsets(word,tag)))\n",
    "                synsets= list(swn.senti_synsets(word,tag))[: n_sin-1]\n",
    "        except UnboundLocalError:\n",
    "            print('Error', list(swn.senti_synsets(word,tag)))\n",
    "        \n",
    "    \n",
    "    \n",
    "    num_synsets= len(synsets)\n",
    "    \n",
    "    if num_synsets == 0:\n",
    "        return {}\n",
    "    \n",
    "    elif num_synsets >0:\n",
    "        for syn in synsets:\n",
    "            #if (syn.pos_score()>lb or syn.neg_score()>lb):\n",
    "            diz['pos']+= syn.pos_score()\n",
    "            diz['neg']+= syn.neg_score()\n",
    "            diz['neutral']+= syn.obj_score()  #* lb / ub\n",
    "                \n",
    "            \"\"\"else:\n",
    "                if num_synsets >=2:\n",
    "                    num_synsets= num_synsets -1   #se una parola ha polarità neutra, non la conto per farci la media..\"\"\"\n",
    "                \n",
    "    for key in diz:\n",
    "        diz[key]/= num_synsets\n",
    "    \n",
    "    norm= sum(diz.values())\n",
    "    \n",
    "    #if norm>0:\n",
    "    \n",
    "    for key in diz:\n",
    "        #diz[key] /= norm\n",
    "        diz[key] = round(diz[key],2)\n",
    "    \n",
    "    #print(word, diz)\n",
    "        \n",
    "    return diz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos': 0.38, 'neg': 0.0, 'neutral': 0.62}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_sentiwordnet('love', 'n', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SentiSynset('knife.n.01'),\n",
       " SentiSynset('knife.n.02'),\n",
       " SentiSynset('tongue.n.03')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(swn.senti_synsets('knife', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "\n",
      "\n",
      "uh RB r\n",
      "drunk JJ a\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "and CC n\n",
      "tired JJ a\n",
      "{'pos': 0.25, 'neg': 0.0, 'neutral': 0.75}\n",
      "\n",
      "\n",
      "and CC n\n",
      "just RB r\n",
      "{'pos': 0.04, 'neg': 0.0, 'neutral': 0.96}\n",
      "\n",
      "\n",
      "remember VBD v\n",
      "{'pos': 0.16, 'neg': 0.0, 'neutral': 0.84}\n",
      "\n",
      "\n",
      "i PRP n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "be VBP v\n",
      "{'pos': 0.06, 'neg': 0.04, 'neutral': 0.9}\n",
      "\n",
      "\n",
      "go VBG v\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "to TO n\n",
      "ireland NNP n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "in IN n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "the DT n\n",
      "morning NN n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "and CC n\n",
      "need VB v\n",
      "{'pos': 0.19, 'neg': 0.19, 'neutral': 0.62}\n",
      "\n",
      "\n",
      "to TO n\n",
      "pack VB v\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      ". . n\n",
      "ireland NNP n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "smile_sad NNP n\n",
      "neutral\n",
      "\n",
      "\n",
      "@theboltonnews NNS n\n",
      "pl NNS n\n",
      "rt SYM n\n",
      "- : n\n",
      "bolton NNP n\n",
      "society NNP n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "for IN n\n",
      "blind NNP n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "people NNS n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "be VBP v\n",
      "{'pos': 0.06, 'neg': 0.04, 'neutral': 0.9}\n",
      "\n",
      "\n",
      "bring VBG v\n",
      "{'pos': 0.0, 'neg': 0.08, 'neutral': 0.92}\n",
      "\n",
      "\n",
      "vegas NNP n\n",
      "{'pos': 0.12, 'neg': 0.0, 'neutral': 0.88}\n",
      "\n",
      "\n",
      "to TO n\n",
      "bolton NNP n\n",
      "with IN n\n",
      "a DT n\n",
      "{'pos': 0.03, 'neg': 0.06, 'neutral': 0.91}\n",
      "\n",
      "\n",
      "charity NNP n\n",
      "{'pos': 0.25, 'neg': 0.0, 'neutral': 0.75}\n",
      "\n",
      "\n",
      "dinner NNP n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "on IN n\n",
      "23 CD n\n",
      "sep NNP n\n",
      "link NNP n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "positive\n",
      "\n",
      "\n",
      "@lennoxlewis NNP n\n",
      "'greatest NNP n\n",
      "slaughter NN n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "in IN n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "the DT n\n",
      "history NN n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "of IN n\n",
      "the DT n\n",
      "apprentice NN n\n",
      "' '' n\n",
      "- : n\n",
      "@realdonaldtrump NN n\n",
      "#celebrityapprenticeusa NN n\n",
      "positive\n",
      "\n",
      "\n",
      "currently RB r\n",
      "in IN n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "egypt NNP n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "& CC n\n",
      "look VBG v\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "4 CD n\n",
      "a DT n\n",
      "{'pos': 0.03, 'neg': 0.06, 'neutral': 0.91}\n",
      "\n",
      "\n",
      "decent JJ a\n",
      "{'pos': 0.71, 'neg': 0.0, 'neutral': 0.29}\n",
      "\n",
      "\n",
      "website NN n\n",
      "to TO n\n",
      "watch VB v\n",
      "{'pos': 0.12, 'neg': 0.0, 'neutral': 0.88}\n",
      "\n",
      "\n",
      "everton NNP n\n",
      "tonight NN n\n",
      "! . n\n",
      "idea NNS n\n",
      "{'pos': 0.06, 'neg': 0.0, 'neutral': 0.94}\n",
      "\n",
      "\n",
      "most RBS r\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "welcome JJ a\n",
      "cheer NNS n\n",
      "{'pos': 0.12, 'neg': 0.0, 'neutral': 0.88}\n",
      "\n",
      "\n",
      "coyb VBP v\n",
      "neutral\n",
      "\n",
      "\n",
      "revis NNP n\n",
      "bat NNS n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "down IN n\n",
      "{'pos': 0.06, 'neg': 0.0, 'neutral': 0.94}\n",
      "\n",
      "\n",
      "sanchez NNP n\n",
      "pass VBP v\n",
      "{'pos': 0.02, 'neg': 0.0, 'neutral': 0.98}\n",
      "\n",
      "\n",
      "to TO n\n",
      "holmes NNP n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      ". . n\n",
      "3rd JJ a\n",
      "pbu NN n\n",
      "by IN n\n",
      "24 CD n\n",
      "in IN n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "2day NNS n\n",
      ". . n\n",
      "holmes NNP n\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "make VBZ v\n",
      "{'pos': 0.03, 'neg': 0.0, 'neutral': 0.97}\n",
      "\n",
      "\n",
      "up RP r\n",
      "{'pos': 0.12, 'neg': 0.06, 'neutral': 0.81}\n",
      "\n",
      "\n",
      "4 CD n\n",
      "it PRP n\n",
      "with IN n\n",
      "catch NN n\n",
      "{'pos': 0.15, 'neg': 0.12, 'neutral': 0.72}\n",
      "\n",
      "\n",
      "on IN n\n",
      "cromartie NNP n\n",
      "on IN n\n",
      "next JJ a\n",
      "{'pos': 0.0, 'neg': 0.0, 'neutral': 1.0}\n",
      "\n",
      "\n",
      "play NN n\n",
      "{'pos': 0.05, 'neg': 0.02, 'neutral': 0.94}\n",
      "\n",
      "\n",
      ". . n\n",
      "#nyj NN n\n"
     ]
    }
   ],
   "source": [
    "lb=0 \n",
    "for i in range(0,5):\n",
    "    parsed_sentence= parsed_sent_array[i]\n",
    "    _class = class_array[i]\n",
    "    \n",
    "    print (_class)\n",
    "    #print (get_tokens(parsed_sentence))\n",
    "    print('\\n')\n",
    "    #print( get_lemmas(parsed_sentence))\n",
    "    #print('\\n')\n",
    "    \"\"\"tokens = get_tokens(parsed_sentence)\n",
    "    tokens = remove_duplicates(tokens)\n",
    "    tokens = lower(tokens)\"\"\"\n",
    "    \n",
    "    ps= parsed_sentence\n",
    "          \n",
    "    indexes = parsed_sentence.get_indexes()\n",
    "    ps= parsed_sentence\n",
    "    # get the surface, lemma and pos for each token\n",
    "    for index in indexes:\n",
    "        #surface = ps.get_surface(index);\n",
    "        lemma = ps.get_lemma(index).lower();\n",
    "        pos = ps.get_postag(index);\n",
    "        print(\"{} {} {}\".format(lemma, pos, penn_to_wn(pos)))\n",
    "        pos_wordnet= penn_to_wn(pos)\n",
    "        sentiword_pol= polarity_sentiwordnet(lemma, pos_wordnet, lb)\n",
    "        if len(sentiword_pol.items())== 3:  #ovvero se ci sono polarity su sentiwordnet\n",
    "            print(polarity_sentiwordnet(lemma, pos_wordnet, lb))\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATION OF POLARITY BASED BAG OF WORD MODEL FOR EACH TWEET \n",
    "\n",
    "1. First I try to look into the lexicon to get the polarity\n",
    "2. If there is no, I will pick it from Sentiwordnet: if the highest polarity among the synonymous is higher than a threhsold(0.5), the average polarity of the synonymous is picked and assigned to the word\n",
    "3. Other BOW representations are built:\n",
    "    * only looking at certain POS structures\n",
    "    * Bigrams-based\n",
    "    * Trigrams-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_out_file = open('C:/Users/feder/Desktop/dev_v4.klp', 'w+', encoding=\"utf-8\")\n",
    "\n",
    "treshold= 0.5\n",
    "stop_words_len= 2\n",
    "sentiwordnet_strategy= 'half'\n",
    "\n",
    "for i in range(0, len(parsed_sent_array)):\n",
    "    parsed_sentence= parsed_sent_array[i]\n",
    "    _class = class_array[i]\n",
    "    \n",
    "    print (_class)\n",
    "    \n",
    "    print( get_lemmas(parsed_sentence))\n",
    "    print('\\n')\n",
    "    tokens = get_tokens(parsed_sentence)\n",
    "    tokens = remove_duplicates(tokens)\n",
    "    tokens = lower(tokens)\n",
    "    \n",
    "    indexes = parsed_sentence.get_indexes()\n",
    "    #print(indexes)\n",
    "    ps= parsed_sentence\n",
    "    # get the surface, lemma and pos for each token\n",
    "    \n",
    "    repr_polar=\"|BV:polb| \";\n",
    "    lemmas= get_lemmas(parsed_sentence)\n",
    "    lemmas= lower(lemmas)\n",
    "    \n",
    "    for index in indexes:\n",
    "        print('fkefkdkfdk', index)\n",
    "        \n",
    "        lemma = ps.get_lemma(index);\n",
    "        pos = ps.get_postag(index);\n",
    "        print(\"{} {} {}\".format(index, lemma, pos))\n",
    "        print('\\n')\n",
    "        \n",
    "        lemma= lemma.lower()\n",
    "        \n",
    "        trial = \"_\"+lemma+\":0.0 \"  #it is the default for each word: polarity= 0.0 means it is neutral\n",
    "        \n",
    "        if len(lemma)> stop_words_len:  # I take off all the words wich have either 1 or 2 letters\n",
    "            if lemma in polaritywords:\n",
    "                print('It is in polarity lexicon')\n",
    "                polarities= polaritywords.get(lemma)\n",
    "                max_polarity= max(np.array(list(map(float, polarities))))\n",
    "\n",
    "                index = np.argmax(np.array(list(map(float, polarities))))\n",
    "                #print('token {}, max polarity {}, index {}'.format(token, float(max_polarity.round(2)), index))\n",
    "            ### IF POLARITY IS MAX, I PUT 1, IF NEGATIVE -1, OTHERWISE 0\n",
    "                if index == 0 and max_polarity> treshold:\n",
    "                    trial= \"_\"+lemma+\":{} \".format(max_polarity)\n",
    "                    print('token {}, max polarity {}, POSITIVE'.format(token, float(max_polarity.round(2))))\n",
    "                elif index == 1 and max_polarity> treshold:\n",
    "                    trial= \"_\"+lemma+\":{} \".format(- max_polarity)\n",
    "                    print('token {}, max polarity {}, NEGATIVE'.format(token, float(-max_polarity.round(2))))\n",
    "\n",
    "            else:  #If word is not in Lexicon, I try in SentiWordnet\n",
    "                pos_wordnet= penn_to_wn(pos)\n",
    "                sentiword_pols= polarity_sentiwordnet(lemma, pos_wordnet, treshold, sentiwordnet_strateg)\n",
    "                if len(sentiword_pols.items())== 3:  #means: if there are polarity (which are 3 for each word)\n",
    "                    max_polarity= max(sentiword_pols.values())\n",
    "                    print('The word is in SentiWordnet',max_polarity)\n",
    "                    if max_polarity > treshold:\n",
    "                        for key in sentiword_pols:\n",
    "                            if sentiword_pols[key]== max_polarity:\n",
    "                                if key== 'pos':\n",
    "                                    trial= \"_\"+lemma+\":{} \".format(max_polarity)\n",
    "                                elif key== 'neg':\n",
    "                                    trial= \"_\"+lemma+\":{} \".format(- max_polarity)\n",
    "\n",
    "            repr_polar += trial\n",
    "    repr_polar +=\"|EV|\"\n",
    "    \n",
    "    \n",
    "    #BUILD a BOW representation of polarity of smiles\n",
    "    repr_pol_smiles=\"|BV:pol_smiles| \";\n",
    "    for lemma in lemmas:\n",
    "        if lemma in smiles:\n",
    "            polarities= polaritywords.get(lemma)\n",
    "            max_polarity= max(np.array(list(map(float, polarities))))\n",
    "            index = np.argmax(np.array(list(map(float, polarities))))\n",
    "            if index == 0 and max_polarity> treshold:\n",
    "                repr_pol_smiles+= \"_\"+lemma+\":{} \".format(max_polarity)\n",
    "            elif index == 1 and max_polarity> treshold:\n",
    "                repr_pol_smiles+= \"_\"+lemma+\":{} \".format(- max_polarity)\n",
    "    repr_pol_smiles +=\"|EV|\"\n",
    "    \n",
    "    \n",
    "    #BUILD a BOW representation with lemmas of only nouns, verbs and adjectives\n",
    "    tokens = get_lemmas(parsed_sentence, \"[n,v,j]\")\n",
    "    tokens = remove_duplicates(tokens)\n",
    "    tokens = lower(tokens)\n",
    "    repr_bow_nvj=\"|BV:bow_nvj| \";\n",
    "    for token in tokens:\n",
    "        repr_bow_nvj += \"_\"+token+\":1.0 \"\n",
    "    repr_bow_nvj +=\"|EV|\"\n",
    "    \n",
    "    \n",
    "    #BUILD a BOW representation with bigrams\n",
    "    \n",
    "    repr_bigrams=\"|BV:bigrams| \";\n",
    "    for j in range(len(lemmas)-1):\n",
    "        current_lemma= lemmas[j]\n",
    "        next_lemma= lemmas[j+1]\n",
    "        repr_bigrams += \"_\"+current_lemma+ \"_\" +next_lemma+\":1.0 \"\n",
    "    repr_bigrams +=\"|EV|\"\n",
    "    \n",
    "    #BUILD a BOW representation with trigrams\n",
    "    \n",
    "    repr_trigrams=\"|BV:trigrams| \";\n",
    "    for j in range(len(lemmas)-2):\n",
    "        current_lemma= lemmas[j]\n",
    "        next_lemma= lemmas[j+1]\n",
    "        third_lemma= lemmas[j+2]\n",
    "        repr_trigrams += \"_\"+current_lemma+ \"_\" +next_lemma+ \"_\" +\n",
    "        third_lemma+\":1.0 \"\n",
    "    repr_trigrams +=\"|EV|\"\n",
    "    \n",
    "    example = _class + \" \" + repr_polar + \" \" + repr_bow_nvj+ \" \" + repr_bigrams+ \" \" + repr_trigrams+ \" \" + repr_pol_smiles\n",
    "    \n",
    "    global_out_file.write(example)\n",
    "    global_out_file.write('\\n')\n",
    "    global_out_file.flush()\n",
    "global_out_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
